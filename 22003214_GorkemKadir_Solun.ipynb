{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YmlJ7FFJGA3k"
   },
   "source": [
    "**<h5></h5>**\n",
    "**<h1><center>CS 464</center></h1>**\n",
    "**<h1><center>Introduction to Machine Learning</center></h1>**\n",
    "**<h1><center>Fall 2024</center></h1>**\n",
    "**<h1><center>Homework 3</center></h1>**\n",
    "<h4><center>Due: December 29, 2024 23:59 (GMT+3)</center></h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kUQigqxM4JlE"
   },
   "source": [
    "## **CIFAR-10 Inpainting**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4JLXD3sTR8Yv"
   },
   "source": [
    "### **Homework Description**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EFNaqLRzD75v"
   },
   "source": [
    "In this assignment, you are asked to design and train a convolutional neural network model for the image inpainting task. In short, inpainting is a process of filling in the missing parts of an image. You will be applying this task on the [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html?ref=assemblyai.com) images. It contains RGB real-life images with the size of 32x32 pixel resolution. You can see a subset of the dataset below. The download link of the dataset is provided in the following parts.\n",
    "\n",
    "![CIFAR-100 Samples](https://drive.google.com/uc?export=view&id=1drp11GJ3QnRivkYLR0nh9RVOi9lnIh8o)\n",
    "\n",
    "**Using PyTorch is mandatory** for this assignment. You are requested to **submit only a single *.ipynb file** in your submissions (no report needed). If you want to provide further explanations about your work, you can add Markdown cells for this purpose. From [this link](https://www.markdownguide.org/), you can get familiar with the Markdown syntax if you need. Upload your homework with the following filename convention: **22003214_GorkemKadir_Solun.ipynb**\n",
    "\n",
    "Note that this assignment needs a CUDA-enabled GPU to be able to train the models in a reasonable time. If you do not have one, it is suggested to use the [Colab](https://colab.research.google.com/) environment.\n",
    "\n",
    "**Contact:** [İpek Öztaş](mailto:ipek.oztas@bilkent.edu.tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rMvuukCeSkRE"
   },
   "source": [
    "### **Importing the Libraries**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bcipuzDyJyMT"
   },
   "source": [
    "In the cell below,  some utilities that you can make use of in this assignment are imported. You can edit these imports considering your implementation as long as you use PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7v0OO4-6SmNV"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vLoc5OoAKDtr"
   },
   "source": [
    "### **Environment Check**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KfGyOffcKG7u"
   },
   "source": [
    "In the cell below, you can test whether hardware acceleration with GPU is enabled in your machine or not. If it is enabled, the printed device should be 'cuda'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KaTYscuOLbjc"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Current device:\", device)\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    print(\"GPU Name:\", torch.cuda.get_device_name(0))\n",
    "    print(\n",
    "        \"Total GPU Memory:\",\n",
    "        round(torch.cuda.get_device_properties(0).total_memory / 1024**3, 1),\n",
    "        \"GB\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BBK2IftRSvHf"
   },
   "source": [
    "### **Setting Library Seeds for Reproducibility**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cOZICZTbMc2-"
   },
   "source": [
    "**DO NOT CHANGE**<br>\n",
    "To make a fair evaluation, the seed values are set for random sampling methods in PyTorch, NumPy, and Python random library. Please do not change these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3M5lcMwQStjy"
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DJ1GUeFfSzN_"
   },
   "outputs": [],
   "source": [
    "seed_everything(464)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h60dtOt5R_aE"
   },
   "source": [
    "### **Preparing the Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dzcz2jwcNKfK"
   },
   "source": [
    "The CIFAR-10 dataset is downloadable from [this link](https://drive.google.com/file/d/1KSKSWiBKvfxBpWmDmuRvUkn77YvWaIFW/view?usp=sharing). If you are using Colab or a Linux machine, you can uncomment and run the below cell to download and extract the dataset automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "85HQtprVSD3p"
   },
   "outputs": [],
   "source": [
    "# NOTE Change the data path to your data path\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "!unzip /content/drive/MyDrive/cifar10_dataset.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZhEiiIYxTBi9"
   },
   "source": [
    "### **Implementing a Custom Dataset [25 Points]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zGpUjWbWOBpJ"
   },
   "source": [
    "In this part, you are requested to implement a custom PyTorch dataset class that reads CIFAR-10 images from a dataset split folder. There are two split folders called train and test in the dataset. The model class should take the root directory of a split in the \\_\\_init\\_\\_ function and read the images accordingly. Before returning the requested images, you should apply the following steps:\n",
    "\n",
    "* Convert images to Tensor object\n",
    "* Normalize tensor values to scale them in the range of (-1,1)\n",
    "\n",
    "Note that reading images in the \\_\\_getitem\\_\\_ function makes the training process slow for this dataset because reading such small-sized images as a batch is slower than the forward pass process of a simple neural network. Therefore, it is suggested to read and store the images in an array in the \\_\\_init\\_\\_ function and return them in the \\_\\_getitem\\_\\_ function when they are requested by the DataLoader object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ongUJiujS_UE"
   },
   "outputs": [],
   "source": [
    "class CifarDataset(Dataset):\n",
    "    def __init__(self, root_dir):\n",
    "        self.classes = sorted(os.listdir(root_dir))\n",
    "        self.labels = [] # Store the labels of the images\n",
    "        self.paths = [] # Store the path of the images\n",
    "        self.transform = transforms.Compose(\n",
    "            [   \n",
    "                # Resize the image to 32x32\n",
    "                transforms.Resize((32, 32)),\n",
    "                # Convert the image to a PyTorch tensor\n",
    "                transforms.ToTensor(),\n",
    "                # Normalize the images with mean and std of the dataset (-1, 1)\n",
    "                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        for i, cls in enumerate(self.classes):\n",
    "            cls_path = os.path.join(root_dir, cls)\n",
    "            for img_name in os.listdir(cls_path):\n",
    "                self.labels.append(i)\n",
    "                self.paths.append(os.path.join(cls_path, img_name))\n",
    "        \n",
    "        self.labels = sorted(self.labels)\n",
    "        self.paths = sorted(self.paths)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, data_id):\n",
    "        img = Image.open(self.paths[data_id])\n",
    "        img = img.convert(\"RGB\")\n",
    "        img = self.transform(img)\n",
    "        # label = self.labels[data_id]\n",
    "        return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sqaS0ePeWzd3"
   },
   "source": [
    "Create a dataset and a data loader object for training and test splits. Set batch sizes to 64 and 512 for training and test data loaders, respectively. Enable shuffling in the training data loader and disable it in the test data loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g-RUAYGdTuVn"
   },
   "outputs": [],
   "source": [
    "train_dataset = CifarDataset(\"/content/train\")\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "test_dataset = CifarDataset(\"/content/test\")\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=512, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XqYREQNbpQeR"
   },
   "source": [
    "**Do not change** the below code. If your implementation is correct, you should be seeing a grid of CIFAR-10 images properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZnVU_VccT-Yi"
   },
   "outputs": [],
   "source": [
    "## Uncomment the cell when the dataloader is ready\n",
    "\n",
    "images = next(iter(train_dataloader))  # Taking one batch from the dataloader\n",
    "images = (images + 1) / 2\n",
    "grid_img = torchvision.utils.make_grid(images[:20], nrow=10)\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(grid_img.permute(1, 2, 0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qx_XrpE44TlW"
   },
   "source": [
    "### **Constructing Convolutional Autoencoder Network [35 Points]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EC8HgZSPmUhS"
   },
   "source": [
    "Autoencoder networks learn how to compress and reconstruct input data. It consists of two networks called the encoder and the decoder. The encoder network compresses the input data, and the decoder network regenerates the data from its compressed version. In this part, you are requested to implement an autoencoder model using convolutional layers. The architecture of the convolutional autoencoder is shown in the below figure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lw4yXKnke3M_"
   },
   "source": [
    "![Convolutional Autoencoder Architecture](https://drive.google.com/uc?export=view&id=19RqHo2PatyoFl7om8OUxBRb1uYAVGmhF)\n",
    "\n",
    "The (in_channel, out_channel) pairs of the layers should be defined as follows:\n",
    "\n",
    "**Encoder:**\n",
    "- (3, 16)\n",
    "- (16, 32)\n",
    "- (32, 64)\n",
    "\n",
    "**Decoder:**\n",
    "- (64, 32)\n",
    "- (32, 16)\n",
    "- (16, 3)\n",
    "\n",
    "You are free to choose the kernel and padding sizes of the layers. In each layer, [2D batch normalization](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html) should be applied and the resulting values should be passed through a LeakyReLU layer with slope 0.2, which is the activation function. Since the image pixel value range is set to [-1,1] in the dataset, the outputs should be bounded so. Therefore, you should be using a Tanh activation function in the last layer instead of the normalization and LeakyReLU layers.\n",
    "\n",
    "In the encoder part of the network, use max pooling in each layer for decreasing the resolution by half. The stride size should be set to one for the convolution layers. In the decoder network, use [transposed convolution](https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html) (deconvolution) layers with stride two for increasing the resolution back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LHSbmwvcUMn6"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.13.1' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/gorke/AppData/Local/Programs/Python/Python313/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "class CifarAutoencoder(nn.Module):\n",
    "    \"\"\"\n",
    "    The (in_channel, out_channel) pairs of the layers should be defined as follows:\n",
    "\n",
    "    **Encoder:**\n",
    "    - (3, 16)\n",
    "    - (16, 32)\n",
    "    - (32, 64)\n",
    "\n",
    "    **Decoder:**\n",
    "    - (64, 32)\n",
    "    - (32, 16)\n",
    "    - (16, 3)\n",
    "\n",
    "    You are free to choose the kernel and padding sizes of the layers. In each layer, [2D batch normalization](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html) should be applied and the resulting values should be passed through a LeakyReLU layer with slope 0.2, which is the activation function. Since the image pixel value range is set to [-1,1] in the dataset, the outputs should be bounded so. Therefore, you should be using a Tanh activation function in the last layer instead of the normalization and LeakyReLU layers.\n",
    "\n",
    "    In the encoder part of the network, use max pooling in each layer for decreasing the resolution by half. The stride size should be set to one for the convolution layers. In the decoder network, use [transposed convolution](https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html) (deconvolution) layers with stride two for increasing the resolution back.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CifarAutoencoder, self).__init__()\n",
    "\n",
    "        # The input size of the encoder should be 3x32x32\n",
    "        # The output size of the encoder is 64x2x2\n",
    "        self.encoder = nn.Sequential(\n",
    "            # Define the encoder layers\n",
    "            nn.Conv2d(3, 16, kernel_size=4, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(16, 32, kernel_size=4, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "\n",
    "        # The output size of the encoder is 64x2x2\n",
    "        # The input size of the decoder should be 64x2x2\n",
    "        self.decoder = nn.Sequential(\n",
    "            # Define the decoder layers\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1, output_padding=0),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=4, stride=2, padding=1, output_padding=0),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.ConvTranspose2d(16, 3, kernel_size=4, stride=2, padding=1, output_padding=0),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # The forward pass of the network\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9YwxmMCrWjtb"
   },
   "source": [
    "### **Implementing the Training Loop [15 Points]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6xMSf3sofmJz"
   },
   "source": [
    "Define your training loop in this function. In the following parts, this function will be called to train the convolutional autoencoder. The input arguments are provided below. Apply the training progress and return a list of losses that are calculated on each epoch. You should sum the iteration losses up during an epoch and take the mean of them to calculate the running loss of that epoch.\n",
    "\n",
    "To be able to learn inpainting, you should mask the input images as follows:\n",
    "\n",
    "![CIFAR Masking](https://drive.google.com/uc?export=view&id=1tlB0mNH4B5dKfokoe162qWgXgPDnOQi2)\n",
    "\n",
    "Simply, you should set the input tensor columns starting from 16 to 32 as -1 (black pixel). For the loss function, you should use the original image as the ground truth image so that the network learns how to fill the masked area of the input image and output the restored image. Before assigning the black pixels, do not forget to clone the original image to use it later in the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cifEP9HEWQde"
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_dataloader, optimizer, loss_func, num_epochs):\n",
    "    model = model.to(device)\n",
    "    epoch_losses = [] # Store the loss value for each epoch\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for images in tqdm(train_dataloader):\n",
    "            images = images.to(device)\n",
    "            # Mask the images by setting the second half of the pixels to -1\n",
    "            masked_images = images.clone()\n",
    "            masked_images[:, :, :, 16:32] = -1\n",
    "\n",
    "            # Forward pass and calculate the loss\n",
    "            outputs = model(masked_images)\n",
    "            loss = loss_func(outputs, images)\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.zero_grad()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        # Calculate the mean loss for the epoch\n",
    "        epoch_loss /= len(train_dataloader)\n",
    "        epoch_losses.append(epoch_loss)\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    return epoch_losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ms6LRikETyds"
   },
   "source": [
    "### **Implementing the Evaluation Function [15 Points]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3XTr6dpe-0NN"
   },
   "source": [
    "Implement an evaluation function that returns the mean MSE calculated over the test dataset samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JW-RK7vDT3l9"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_dataloader):\n",
    "    total_loss = 0.0\n",
    "    num_images = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images in tqdm(test_dataloader):\n",
    "            # Mask the images by setting the second half of the pixels to -1\n",
    "            masked_images = images.clone()\n",
    "            masked_images[:, :, :, 16:32] = -1\n",
    "\n",
    "            outputs = model(masked_images) # Get the output from the model\n",
    "            loss = nn.MSELoss(outputs, images) # Calculate the MSE loss\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            num_images += len(images)\n",
    "\n",
    "    # Return the mean loss of the test dataset\n",
    "    return total_loss / num_images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UjkRLMF0ZCtk"
   },
   "source": [
    "### **Inpainting Visualization Function**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hI4ZNkVq_onm"
   },
   "source": [
    "The below code will be used to visualize the outputs of the trained models later. **Do not change the codes in the cell**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VAuNmuIelqzI"
   },
   "outputs": [],
   "source": [
    "def visualize_inpainting(model, dataset):\n",
    "    seed_everything(464)\n",
    "    dataloader = DataLoader(dataset, batch_size=10, shuffle=True)\n",
    "    images = next(iter(dataloader))  # Taking one batch from the dataloader\n",
    "    images = images\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        masked_images = images.clone()\n",
    "        masked_images[:, :, :, 16:] = -1\n",
    "        inpainted_images = model(masked_images.cuda()).cpu()\n",
    "    images = (images + 1) / 2\n",
    "    masked_images = (masked_images + 1) / 2\n",
    "    inpainted_images = (inpainted_images + 1) / 2\n",
    "    images_concat = torch.cat((images, masked_images, inpainted_images), dim=2)\n",
    "    grid_img = torchvision.utils.make_grid(images_concat, nrow=10)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(grid_img.permute(1, 2, 0))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0JOGHIOakmL-"
   },
   "source": [
    "### **Training and Evaluating the Model [10 Points]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lYdoryMh_9HN"
   },
   "source": [
    "Define your loss function as MSE, set learning rate to 2e-4, create Adam optimizer, and set number of epochs to 50. Later, call the train_model function that you implemented. Visualize the returned losses on a plot (loss vs. epoch). Lastly, call evaluate_model function that you implemented and print the mean square error that your model reached on the test dataset. Also, call the visualize_inpainting function to observe the final inpainting results on the test dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8JhkdYbwiw0s"
   },
   "outputs": [],
   "source": [
    "seed_everything(464)\n",
    "model = CifarAutoencoder() # Create the model\n",
    "model = model.to(device)\n",
    "\n",
    "train_losses = train_model(model, train_dataloader, optim.Adam(model.parameters(), lr=2e-4), nn.MSELoss(), 50)\n",
    "\n",
    "# Plot the training loss vs epoch\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, 51), train_losses, label='Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss vs Epochs')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Test MSE: {evaluate_model(model, test_dataloader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IgiD1WG-ZRgf"
   },
   "outputs": [],
   "source": [
    "visualize_inpainting(model, test_dataset) ## Uncomment when the model is trained"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
